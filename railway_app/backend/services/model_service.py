"""
Model Service - Gerenciamento de modelos LSTM
Deploy auto-contido para Railway

Suporta:
- Modelos originais (LSTMPredictor)
- Modelos melhorados (ImprovedLSTMPredictor) com Attention
"""
from pathlib import Path
from typing import Dict, Optional, List
import pandas as pd
import numpy as np
import torch
from loguru import logger

# Importar de core/ (copia local para deploy)
from core.lstm_model import LSTMPredictor
from core.improved_lstm import ImprovedLSTMPredictor, detect_model_type
from core.preprocessor import StockDataPreprocessor

# HuggingFace Hub para baixar modelos
from huggingface_hub import hf_hub_download


class ModelService:
    """Servico de gerenciamento de modelos LSTM."""
    
    HUB_REPO = "henriquebap/stock-predictor-lstm"
    LOCAL_CACHE = Path("models")
    
    # Modelos dispon√≠veis no Hub (atualizados)
    AVAILABLE_MODELS = [
        "BASE", "AAPL", "GOOGL", "MSFT", "AMZN", 
        "META", "NVDA", "TSLA", "JPM", "V"
    ]
    
    def __init__(self):
        self.model_cache: Dict[str, dict] = {}
        self.LOCAL_CACHE.mkdir(exist_ok=True)
        logger.info(f"üß† ModelService inicializado | Hub: {self.HUB_REPO}")
    
    def _download_from_hub(self, filename: str) -> Path:
        """Baixa arquivo do HuggingFace Hub."""
        logger.info(f"üì• Baixando do Hub: {filename}")
        path = Path(hf_hub_download(
            repo_id=self.HUB_REPO,
            filename=filename,
            cache_dir=str(self.LOCAL_CACHE / "hub_cache")
        ))
        logger.info(f"‚úÖ Download concluido: {filename}")
        return path
    
    def _load_model(self, symbol: str) -> Optional[dict]:
        """Carrega modelo do cache local ou Hub."""
        model_file = f"lstm_model_{symbol}.pth"
        scaler_file = f"scaler_{symbol}.pkl"
        
        logger.info(f"üîç Procurando modelo para {symbol}...")
        
        # Tentar cache local primeiro
        local_model = self.LOCAL_CACHE / model_file
        local_scaler = self.LOCAL_CACHE / scaler_file
        
        if local_model.exists() and local_scaler.exists():
            model_path = local_model
            scaler_path = local_scaler
            source = "local"
            logger.info(f"üìÅ Modelo LOCAL encontrado para {symbol}")
        else:
            # Tentar HuggingFace Hub - modelo especifico
            logger.info(f"üåê Buscando modelo {symbol} no HuggingFace Hub...")
            try:
                model_path = self._download_from_hub(model_file)
                scaler_path = self._download_from_hub(scaler_file)
                source = "hub"
                logger.info(f"‚úÖ Modelo para {symbol} encontrado no Hub!")
            except Exception as e:
                # Fallback para modelo BASE
                logger.warning(f"‚ö†Ô∏è Modelo espec√≠fico para {symbol} n√£o encontrado: {e}")
                logger.info(f"üîÑ Usando modelo BASE gen√©rico...")
                try:
                    model_path = self._download_from_hub("lstm_model_BASE.pth")
                    scaler_path = self._download_from_hub("scaler_BASE.pkl")
                    source = "base"
                    logger.info(f"‚úÖ Modelo BASE carregado para {symbol}")
                except Exception as e2:
                    logger.error(f"‚ùå Falha ao carregar modelo BASE: {e2}")
                    return None
        
        # Detectar tipo do modelo e carregar
        try:
            model_type = detect_model_type(model_path)
            logger.info(f"üîé Tipo detectado: {model_type}")
            
            if model_type == 'improved':
                model = ImprovedLSTMPredictor.load(model_path)
                logger.info(f"‚úÖ Carregado como ImprovedLSTMPredictor")
            else:
                model = LSTMPredictor.load(model_path)
                logger.info(f"‚úÖ Carregado como LSTMPredictor")
            
            preprocessor = StockDataPreprocessor.load(scaler_path)
            
            logger.info(f"üéØ Modelo carregado | Symbol: {symbol} | Source: {source} | Type: {model_type}")
            
            return {
                'model': model,
                'preprocessor': preprocessor,
                'source': source,
                'model_type': model_type,
                'symbol_requested': symbol
            }
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            return None
    
    def get_model(self, symbol: str) -> Optional[dict]:
        """Obtem modelo do cache ou carrega."""
        symbol = symbol.upper()
        
        if symbol not in self.model_cache:
            logger.info(f"üì¶ Modelo {symbol} n√£o est√° em cache, carregando...")
            model_data = self._load_model(symbol)
            if model_data:
                self.model_cache[symbol] = model_data
        else:
            logger.info(f"‚ö° Modelo {symbol} encontrado em cache!")
        
        return self.model_cache.get(symbol)
    
    def predict(self, symbol: str, df: pd.DataFrame) -> dict:
        """Faz previsao para um simbolo."""
        logger.info(f"üîÆ Iniciando previs√£o para {symbol}...")
        
        # Tentar modelo especifico, depois BASE
        model_data = self.get_model(symbol)
        
        if not model_data:
            logger.warning(f"‚ö†Ô∏è Nenhum modelo dispon√≠vel para {symbol}, tentando BASE...")
            model_data = self.get_model("BASE")
        
        if not model_data:
            # Fallback: media movel simples
            logger.warning(f"‚ö†Ô∏è Usando FALLBACK (m√©dia m√≥vel) para {symbol}")
            current = float(df['close'].iloc[-1])
            momentum = float((df['close'].iloc[-1] - df['close'].iloc[-5]) / df['close'].iloc[-5])
            predicted = current * (1 + momentum * 0.3)
            
            return {
                'predicted_price': predicted,
                'model_type': '‚ö†Ô∏è Fallback (M√©dia M√≥vel)'
            }
        
        model = model_data['model']
        preprocessor = model_data['preprocessor']
        source = model_data['source']
        arch_type = model_data.get('model_type', 'unknown')
        
        # Fazer previsao
        try:
            X = preprocessor.transform_for_prediction(df)
            predictions = model.predict(X)
            pred_scaled = predictions[0]
            predicted_price = preprocessor.inverse_transform_target(pred_scaled)
            
            # Determinar nome do modelo para exibi√ß√£o (CORRIGIDO - sem "Fine-tuned")
            if source == "hub" or source == "local":
                if arch_type == 'improved':
                    model_type_display = f"üéØ LSTM Espec√≠fico ({symbol})"
                else:
                    model_type_display = f"üìä LSTM ({symbol})"
            elif source == "base":
                model_type_display = "üß† LSTM Base (gen√©rico)"
            else:
                model_type_display = f"LSTM ({source})"
            
            logger.info(f"‚úÖ Previs√£o conclu√≠da | {symbol} | ${predicted_price:.2f} | {model_type_display}")
            
            return {
                'predicted_price': float(predicted_price),
                'model_type': model_type_display
            }
        except Exception as e:
            logger.error(f"‚ùå Erro na previs√£o para {symbol}: {e}")
            current = float(df['close'].iloc[-1])
            return {
                'predicted_price': current,
                'model_type': f'‚ö†Ô∏è Erro: {str(e)[:30]}'
            }
    
    def list_available_models(self) -> List[str]:
        """Lista modelos disponiveis."""
        models = list(self.AVAILABLE_MODELS)
        
        # Adicionar modelos locais
        for f in self.LOCAL_CACHE.glob("lstm_model_*.pth"):
            name = f.stem.replace("lstm_model_", "")
            if name not in models:
                models.append(name)
        
        return models
    
    def clear_cache(self, symbol: Optional[str] = None):
        """Limpa cache de modelos."""
        if symbol:
            symbol = symbol.upper()
            if symbol in self.model_cache:
                del self.model_cache[symbol]
                logger.info(f"üóëÔ∏è Cache limpo para {symbol}")
        else:
            self.model_cache.clear()
            logger.info("üóëÔ∏è Cache completo limpo")
